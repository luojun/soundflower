Please generate project files according to @prompt.txt .

~~
Please create a git ignore file appropriate for this project.

~~
Please add visualization. Please consider keeping the visualization code
in a separate package.

~~
Please actually do animation -- animated rendering of the environment
dynamics, including the robotic arm's motion and the sound source motion.
Maybe consider using Pygame?

~~
Currently the @agents/heuristic_agent.py implementation has the HeristicAgent
holding a reference to the environment object, which is less than ideal. It
needs that for the "run_episode" method. However, the run_episode method
really belongs elsewhere. Suggestion: please refactor the code such that we
 have an "experiments" package under which methods such as run_episode could
 live. Similarly, the repetitive code in @demo.py @demo_pygame.py and
 @demo_visualization.py could be refactored into an Experiment class under
 the experiments package and thus greatly simplfied.
~
Note that in @demo_pygame.py @demo.py and @demo_visualization.py ,
create_default_config was repeated three times. This method really belongs
to @experiments/experiment.py . Furthermore, please reconsider how experiment
level config should be used by the SoundFlowerEnvironment class and the
HeuristicAgent class. For now, it's probably OK to expose the config to them
directly. Please refactor further.

~~
We need to rectify how physics simulation is done, how it is coupled with
visualization, and how it is coupled with agent observation and action.
At a high level, the physics simulation cycle, the visualization cycle,
and the agent cycle (observation to decision) should all be asynchronoous.
These are then flexibly coupled under adjustable freqencies. The physics
simulation is to be governed by a changeable time step size, which is how
it discretize in time the underlying continuous physics. Then the simulation
frequency depends on how fast the simulation computation could run. This
frequency may be left unknown to be determined empirically on actual
computers. The visualization frequency needs to be suitable for human
observers, which means it is to be flexible in an acceptable range, such as
between 10fps and 100fps, the specific value could be picked according to
how fast the rendering engine (e.g. Pygame here) could run with respect to
the physics engine. I.e. there could be a somewhat flexible mapping between
fps and time step size of physical simulation. The agent interaction
frequency needs to be suitable for how fast the agent decision could run
(the mapping from observation to action). We can assume it's somewhere
between 10hz to 100hz. We can also call this the "control frequency".
Please refactor the code to (1) introduce a proper (2D) physics engine to
run the physical simulation with changeable time step size, (2) use
flexible control frequency to integrate agent's interaction with the 
physical environment, (3) allow a headless mode with integrated physics
simulation and agent control to run much faster than real time, and (4)
allow a flexible visualization frequency to watch the simulation at any
speed-up ratio that is reasonable.
~
Can you study this repo: https://github.com/luojun/OmegaZero/tree/master ?
To see whether there's anything to borrow from there? Maybe look into the
Runner, Renderer, and World (Environment) differentiation and interfadces? 

~~
Please (1) remove the version of visualization that uses matplotlib,
including demo_visualization.py and the underlying matplotlib implementation,
(2) remove demo_pygame.py along with the old pygame_visualizer.py,
(3) renaming the visulazation package to animation and change corresponding
namings for the implementation, and (4) rename demo_decoupled.py to
demo_animation.py.

~~
Why do you think we need both @demo.py and @demo_animation.py ? Can't we
use a single demo implementation? We already have a headless mode.

~~
Rename the @animation package to animator.

~~
Let's do a round of cleanup and refactoring: (1) Move create_default_config
in experiment.py into config.py in the soundflower package; (2) Remove 
experimemnt.py and simulation_experiment.py; (3) Rename the experiments
package into "experimenter"; (4) Move "config.py" and "runner.py" from 
the soundflower package into the renamed "experimenter" package; (5) Move 
the Observation dataclass from environment.py into world.py; (6) Remove
the old environment.py; (7) Create a new package called "environment";
(8) Move world.py, physics.py, and physics_engine.py into this new
environment package; (9) Move renderer.py into the animator package;
(10) Rename pygame_animator into pygame_framer, and adjust the references
to it as well as the classes it implements appriopriately.
~
Update README.md and ARCHITECTURE.md to reflect the current state of the code.
~
Remove soundflower/world.py and soundflower/config.py.
~
Remove simulation_loop.py, move world.py from the environment package
into the soundflower package, update all references in README.md if any.
Remove ARCHITECTURE.md, update all references to it if any.

#~~
#Did a bit of refactoring by hand to move Animator under Experimenter.
#~
#Refactoring by hand: renamed World into Environment and moved under environment.
#~
#Refactor by hand: simplify demo startup.
#~
#Refactor by hand: encapsulate animator initialization.
#~
#Refactor by hand: move experiment orchestration into runner.
#~
#Refacotr by hand: introduce logger and simplify integration by removing asyncio

~
Implement advanced simulation stepping features (through primarily changing @soundflower.py and @demo.py)
in the following way:
1. When the Return key is pressed, the simulation pauses and steps forward by
a single step, and logging and animation are all refreshed once by that one step.
After that, the simulation remains in a paused state.
2. When the '1' key is pressed, the behavior follows the same logic, except that the simulation
advances by 10 steps, and logging and animtation are all frefreshed once by that 10 steps.
3. When the '2' key is pressed, the same things, except that the simulation advances by 100 steps.
4. When the '3' key is pressed, the same thing by 1000 steps.
5. If the Space key is pressed while the simulation is paused, the whole simulation resumes normally
along with logging and animation, as how things are now.

~
Update the code to calculate and report performance better in the following way:
1. Replace wherever appropriate (e.g. in @physics.py) "sound_energy" by "sound_intensity";
2. Introduce specific converstion of sound_intensity to sound_energy by (2a) assuming
a unit area and (2b) multipilcation by time step size, i.e. dt.
3. Introduce orientation angle for the "microphone" (of the unit area) that receives
the sound such that the sound_intensity's conversion into sound_energy is a function of
the orientation angle, such as proportional to the cosine of difference between the
orientation angle and the direction of the sound source, or following whatever simple
but sensible physics. The orientation of the microphone should be in the direction of
the outmost link from the soundflower robot's base. Add up sound energy from multiple sources.
4. Introduce a new variable in @soundflower.py called "cumulative_sound_energy" and update it
according to the accumulation over time of sound_energy by the microphone.
5. Update logging for the final step to also report the cumulative_sound_energy.
6. Update logging per step to make the logged text easier to read with appropriate formatting
and include the simulation_time at that step as well as the step_count.
7. Remove simulation_time and step_count from @physics_engine.py and make sure that only
@soundflower.py tracks these.

~
Update the code to
1. Remove the Info part from get_state in @environment.py;
2. Make per step logging report reward rather than energy delta;
3. Use appropriate units for energy;
4. Make sure logging of float values use only two digits after the decimal point;
5. Remove _compute_distance_to_nearest_source and _compute_orientation_factors.

~
We need to fix the bug where the agent could accumulate more than 1 million Joules
of energy within 160 seconds even when the sound source is at most 2 Watts.
Concretely, (1) verify and ensure the 4π factor is explicitly included in the
inverse square law calculation (intensity = P / (4πr²)); (2) verify and ensure
the microphone surface area is set to a physically reasonable value (e.g., 1 cm²
or 0.0001 m²) rather than an unrealistically large value; (3) enforce a minimum
distance constraint of 0.2 meters between the microphone and any sound source as
a physical limit, meaning the arm's end effector position must be constrained
such that it cannot physically reach within 0.2 meters of any sound source
position, both during initialization and throughout the simulation; (4) implement
this physical constraint by modifying the arm's reachable workspace or by
constraining the forward kinematics to respect this minimum distance; and (5)
implement or enhance the test in test_energy_plausibility.py to verify that
cumulative sound energy remains physically plausible (i.e., total energy cannot
exceed source_power × simulation_time, and should be significantly less due to
distance attenuation, the 4π factor, and orientation effects).

~
Refactor and extend the agent system to support multiple tracking strategies:
(1) Refactor @HeuristicAgent to extract shared components (PD controller, target angle
computation, inverse kinematics) into a base class or shared utility functions that
can be reused across multiple agent implementations.
(2) Create three distinct agent classes with different tracking behaviors:
    - PointingAgent: Only orients the microphone toward the sound source without
      attempting to minimize distance (maintains current distance while pointing).
    - ApproachingAgent: Only tries to minimize distance to the sound source
      without explicit orientation control (the current HeuristicAgent behavior).
    - TrackingAgent: Both points toward and minimizes distance to the sound source,
      combining orientation and distance objectives.
(3) Update @demo.py to use the TrackingAgent instead of the current HeuristicAgent.
(4) Create @demo_all.py that runs all three agents simultaneously in separate
simulation instances, each with its own logging and animation visualization, allowing
side-by-side comparison of the different tracking strategies.
~
Update the logging of energy, reward and total reward to 4 positions after the decimal
point.
~
Update logging so that the specific simulation instance (e.g. those launched in
@demo_all) is identified according to the type of agent.

~
Normalize the reward signal to improve numerical stability and make it more suitable
for reinforcement learning algorithms. Currently, the reward is the sound energy delta,
which has very small numerical values (on the order of 1e-6 Joules per step).
Implement reward normalization as follows:
(1) Add a `reward_normalization_factor` parameter to @SoundFlowerConfig with a default
value of None (auto-computed). In the `__post_init__` method, if this factor is None,
automatically compute it as the theoretical maximum possible energy delta per step:
max_intensity = sound_source_strength / (4π × min_distance_to_source²)
max_energy_per_step = max_intensity × microphone_area × dt × 1.0
reward_normalization_factor = max_energy_per_step
(2) In @Environment.get_state(), normalize the reward by dividing sound_energy_delta
by the reward_normalization_factor before returning it in the State. This will scale
the reward to approximately [-1, 1] range (though it may exceed 1.0 if multiple
sources align perfectly or if the agent moves from zero to maximum energy in a single
step). Ensure the normalization factor is greater than zero before dividing to avoid
division by zero errors.

~
Add a plotter component for real-time visualization of simulation metrics, following
the design pattern of @logger and @animator. The plotter should:
(1) Use matplotlib for plotting and follow the same interface pattern as logger/animator
    (with __init__, start(), step(), and finish() methods) and integrate similarly
    into the simulation loop in @soundflower.py.
(2) Be designed as a shared instance that can track multiple simulation instances
    simultaneously (e.g., in @demo_all.py where multiple agents run in parallel).
    The plotter should accept an agent_name parameter to identify which simulation
    instance each data point belongs to.
(3) Create four separate plots that update in real-time as simulations progress:
    - Per-step reward (normalized)
    - Per-step sound energy (raw, in Joules)
    - Cumulative reward (sum of normalized rewards)
    - Cumulative sound energy (sum of raw energy, in Joules)
(4) For each plot, display multiple time series corresponding to different agents
    (e.g., PointingAgent, ApproachingAgent, TrackingAgent). Use distinct line colors
    and line styles (solid, dashed, dotted) to differentiate between agent types.
    Maintain a consistent color/style mapping across all four plots for the same agent.
(5) Handle normalization appropriately: use normalized reward values (already computed
    in the environment) for reward plots, and raw energy values for energy plots.
(6) Update plots incrementally as new data arrives (not batch updates at the end).
    Use matplotlib's interactive mode (plt.ion()) and refresh the plots at a
    configurable frequency (similar to animation_frequency in config). Ensure plots
    remain responsive and visible throughout the simulation run.
~
Pleaes also update @demo to use @plotter.

~
Refactor and improve the agent system to address critical issues in decision-making and
inverse kinematics. The current implementation has several problems that prevent agents
from effectively exploiting their degrees of freedom and differentiating their behaviors:

(1) Remove @HeuristicAgent as it is redundant. ApproachingAgent already implements the same
    behavior using the BaseAgent architecture. Update @test_energy_plausibility.py to use
    ApproachingAgent instead of HeuristicAgent, remove HeuristicAgent from @agents/__init__.py
    exports, and remove any references to HeuristicAgent in the plotter color/style mappings.

(2) Replace the simplistic inverse kinematics with proper position-based IK. The current
    `_compute_desired_joint_angles` method in @BaseAgent only distributes a target angle
    across joints with fixed ratios (0.7/0.3 for 2 links, 0.5/0.3/0.2 for 3 links), which
    doesn't actually solve for joint angles to reach a target position. This severely
    limits the agents' ability to exploit their degrees of freedom and causes all agents
    to behave similarly. Implement a proper Jacobian-based inverse kinematics solver in
    BaseAgent that:
    - Takes a target (x, y) position for the end effector
    - Uses numerical Jacobian computation and pseudo-inverse to solve for joint angles
    - Includes damping/regularization for stability
    - Works for both 2-link and 3-link arms
    - Has configurable convergence tolerance and maximum iterations

(3) Redesign agents to use position-based objectives instead of angle-based ones:
    - PointingAgent: Should optimize orientation toward the sound source while ignoring
      distance to target (distance is not subject to IK optimization pressure, though it
      may change naturally as a side effect). Compute target position by projecting the
      source direction onto a circle centered at base with radius equal to current
      end effector distance, then solve IK to reach that position.
    - ApproachingAgent: Should minimize distance to sound source while ignoring target
      orientation (orientation is not subject to IK optimization pressure, though it may
      change naturally as a side effect). Compute target position by moving toward the
      source with an adaptive step size (respecting minimum distance constraint), then
      solve IK to reach that position.
    - TrackingAgent: Should optimize both objectives simultaneously. Compute a weighted
      combination of the pointing target position and approaching target position, then
      solve IK to reach the combined target.

(4) Pass link_lengths to agents for IK computation. The IK solver needs link lengths to
    compute forward kinematics. Either add link_lengths as a parameter to BaseAgent.__init__
    (passed from config), or add a method to access them from the observation/environment.
    Ensure the IK solver uses the actual link lengths from the configuration.

(5) Consider additional improvements for better control:
    - Add adaptive PD gains that scale with distance to target or link lengths
    - For 3-link arms, consider null-space control to optimize secondary objectives
    - Add velocity feedforward terms for smoother motion
    - Consider joint limit constraints in the IK solver

These changes will enable agents to properly differentiate their behaviors, exploit the full
workspace, and scale effectively to 3-link arms while maintaining the shared code structure
in BaseAgent.

~
Fix a critical bug in the inverse kinematics solver where the first link (and potentially other
links) fails to move when its angle is between 180 and 360 degrees (i.e., pointing westward
through eastward). This bug affects all three agent types (PointingAgent, ApproachingAgent,
TrackingAgent) and prevents proper arm control in certain orientations. The root causes are:

(1) Angle clamping during IK iterations: In `_solve_inverse_kinematics` in @BaseAgent, angles
    are clamped to [-π, π] after each iteration (line 210: `angles = np.clip(angles, -np.pi, np.pi)`).
    This creates discontinuities when angles need to cross the ±π boundary. For example, if the
    current angle is 3.0 rad (172°) and needs to evolve to 3.5 rad (200°), the clamp wraps it
    to -2.78 rad, breaking the iterative convergence process.

(2) Missing angle wrapping in PD controller: In `_compute_pd_torques` in @BaseAgent, angle errors
    are computed without wrapping (line 229: `angle_errors = desired_angles - current_angles`).
    When angles wrap around the ±π boundary, this produces incorrect error values. For example,
    if desired_angle is -2.78 rad and current_angle is 3.0 rad, the error is -5.78 rad instead
    of the correct wrapped value of approximately 0.5 rad.

Implement the following fixes:

(1) Remove clamping during IK iterations: In `_solve_inverse_kinematics`, remove the `np.clip`
    call inside the iteration loop (line 210). Instead, normalize angles to [-π, π] only at the
    end of the function, after all iterations complete, using `angles = np.arctan2(np.sin(angles),
    np.cos(angles))`. This allows angles to evolve naturally during iterations without artificial
    discontinuities.

(2) Normalize input angles at IK start: At the beginning of `_solve_inverse_kinematics`, after
    copying `current_angles` to `angles`, normalize the initial angles to [-π, π] for consistency:
    `angles = np.arctan2(np.sin(angles), np.cos(angles))`.

(3) Add angle wrapping to PD controller: In `_compute_pd_torques`, after computing `angle_errors`,
    wrap the errors to [-π, π] using `angle_errors = np.arctan2(np.sin(angle_errors),
    np.cos(angle_errors))`. This ensures that angle errors are always computed as the shortest
    angular distance, correctly handling wrap-around cases.

These fixes will ensure that the IK solver can handle all angle ranges correctly and that the
PD controller computes appropriate torques regardless of the current joint angles' positions
relative to the ±π boundary.

~
Extend the inverse kinematics solver to explicitly support both position and orientation objectives,
and update all agents to use this unified interface. Energy maximization depends on both distance
(inverse square law) and orientation (cosine factor), so agents must optimize both independently or
in combination. Extend _solve_inverse_kinematics in @agents/base_agent.py to accept optional
target_orientation and weights (position_weight, orientation_weight), allowing position-only,
orientation-only, or combined control. Update PointingAgent to use orientation-only IK
(position_weight=0.0), ApproachingAgent to use position-only IK (orientation_weight=0.0),
and TrackingAgent to use both with appropriate weights. Rename variables in
@environment/environment.py for clarity where appropriate.
~
Update the argument order of _solve_inverse_kinematics and other related functions to a more
natural and appropriate order. Do review the variable names to see which ones should be updated
too. And no trailing whitespaces please.

~
Fix the issue where the keys '1', '2', '3', '4' no longer seem to have effect for @demo_all.
Note that these keys still works in @demo.

