Please generate project files according to @prompt.txt .

~~
Please create a git ignore file appropriate for this project.

~~
Please add visualization. Please consider keeping the visualization code
in a separate package.

~~
Please actually do animation -- animated rendering of the environment
dynamics, including the robotic arm's motion and the sound source motion.
Maybe consider using Pygame?

~~
Currently the @agents/heuristic_agent.py implementation has the HeristicAgent
holding a reference to the environment object, which is less than ideal. It
needs that for the "run_episode" method. However, the run_episode method
really belongs elsewhere. Suggestion: please refactor the code such that we
 have an "experiments" package under which methods such as run_episode could
 live. Similarly, the repetitive code in @demo.py @demo_pygame.py and
 @demo_visualization.py could be refactored into an Experiment class under
 the experiments package and thus greatly simplfied.
~
Note that in @demo_pygame.py @demo.py and @demo_visualization.py ,
create_default_config was repeated three times. This method really belongs
to @experiments/experiment.py . Furthermore, please reconsider how experiment
level config should be used by the SoundFlowerEnvironment class and the
HeuristicAgent class. For now, it's probably OK to expose the config to them
directly. Please refactor further.

~~
We need to rectify how physics simulation is done, how it is coupled with
visualization, and how it is coupled with agent observation and action.
At a high level, the physics simulation cycle, the visualization cycle,
and the agent cycle (observation to decision) should all be asynchronoous.
These are then flexibly coupled under adjustable freqencies. The physics
simulation is to be governed by a changeable time step size, which is how
it discretize in time the underlying continuous physics. Then the simulation
frequency depends on how fast the simulation computation could run. This
frequency may be left unknown to be determined empirically on actual
computers. The visualization frequency needs to be suitable for human
observers, which means it is to be flexible in an acceptable range, such as
between 10fps and 100fps, the specific value could be picked according to
how fast the rendering engine (e.g. Pygame here) could run with respect to
the physics engine. I.e. there could be a somewhat flexible mapping between
fps and time step size of physical simulation. The agent interaction
frequency needs to be suitable for how fast the agent decision could run
(the mapping from observation to action). We can assume it's somewhere
between 10hz to 100hz. We can also call this the "control frequency".
Please refactor the code to (1) introduce a proper (2D) physics engine to
run the physical simulation with changeable time step size, (2) use
flexible control frequency to integrate agent's interaction with the 
physical environment, (3) allow a headless mode with integrated physics
simulation and agent control to run much faster than real time, and (4)
allow a flexible visualization frequency to watch the simulation at any
speed-up ratio that is reasonable.
~
Can you study this repo: https://github.com/luojun/OmegaZero/tree/master ?
To see whether there's anything to borrow from there? Maybe look into the
Runner, Renderer, and World (Environment) differentiation and interfadces? 

~~
Please (1) remove the version of visualization that uses matplotlib,
including demo_visualization.py and the underlying matplotlib implementation,
(2) remove demo_pygame.py along with the old pygame_visualizer.py,
(3) renaming the visulazation package to animation and change corresponding
namings for the implementation, and (4) rename demo_decoupled.py to
demo_animation.py.

~~
Why do you think we need both @demo.py and @demo_animation.py ? Can't we
use a single demo implementation? We already have a headless mode.

~~
Rename the @animation package to animator.

~~
Let's do a round of cleanup and refactoring: (1) Move create_default_config
in experiment.py into config.py in the soundflower package; (2) Remove 
experimemnt.py and simulation_experiment.py; (3) Rename the experiments
package into "experimenter"; (4) Move "config.py" and "runner.py" from 
the soundflower package into the renamed "experimenter" package; (5) Move 
the Observation dataclass from environment.py into world.py; (6) Remove
the old environment.py; (7) Create a new package called "environment";
(8) Move world.py, physics.py, and physics_engine.py into this new
environment package; (9) Move renderer.py into the animator package;
(10) Rename pygame_animator into pygame_framer, and adjust the references
to it as well as the classes it implements appriopriately.
~
Update README.md and ARCHITECTURE.md to reflect the current state of the code.
~
Remove soundflower/world.py and soundflower/config.py.
~
Remove simulation_loop.py, move world.py from the environment package
into the soundflower package, update all references in README.md if any.
Remove ARCHITECTURE.md, update all references to it if any.

#~~
#Did a bit of refactoring by hand to move Animator under Experimenter.
#~
#Refactoring by hand: renamed World into Environment and moved under environment.
#~
#Refactor by hand: simplify demo startup.
#~
#Refactor by hand: encapsulate animator initialization.
#~
#Refactor by hand: move experiment orchestration into runner.
#~
#Refacotr by hand: introduce logger and simplify integration by removing asyncio

~
Implement advanced simulation stepping features (through primarily changing @soundflower.py and @demo.py)
in the following way:
1. When the Return key is pressed, the simulation pauses and steps forward by
a single step, and logging and animation are all refreshed once by that one step.
After that, the simulation remains in a paused state.
2. When the '1' key is pressed, the behavior follows the same logic, except that the simulation
advances by 10 steps, and logging and animtation are all frefreshed once by that 10 steps.
3. When the '2' key is pressed, the same things, except that the simulation advances by 100 steps.
4. When the '3' key is pressed, the same thing by 1000 steps.
5. If the Space key is pressed while the simulation is paused, the whole simulation resumes normally
along with logging and animation, as how things are now.

~
Update the code to calculate and report performance better in the following way:
1. Replace wherever appropriate (e.g. in @physics.py) "sound_energy" by "sound_intensity";
2. Introduce specific converstion of sound_intensity to sound_energy by (2a) assuming
a unit area and (2b) multipilcation by time step size, i.e. dt.
3. Introduce orientation angle for the "microphone" (of the unit area) that receives
the sound such that the sound_intensity's conversion into sound_energy is a function of
the orientation angle, such as proportional to the cosine of difference between the
orientation angle and the direction of the sound source, or following whatever simple
but sensible physics. The orientation of the microphone should be in the direction of
the outmost link from the soundflower robot's base. Add up sound energy from multiple sources.
4. Introduce a new variable in @soundflower.py called "cumulative_sound_energy" and update it
according to the accumulation over time of sound_energy by the microphone.
5. Update logging for the final step to also report the cumulative_sound_energy.
6. Update logging per step to make the logged text easier to read with appropriate formatting
and include the simulation_time at that step as well as the step_count.
7. Remove simulation_time and step_count from @physics_engine.py and make sure that only
@soundflower.py tracks these.

~
Update the code to
1. Remove the Info part from get_state in @environment.py;
2. Make per step logging report reward rather than energy delta;
3. Use appropriate units for energy;
4. Make sure logging of float values use only two digits after the decimal point;
5. Remove _compute_distance_to_nearest_source and _compute_orientation_factors.

~
We need to fix the bug where the agent could accumulate more than 1 million Joules
of energy within 160 seconds even when the sound source is at most 2 Watts.
Concretely, (1) verify and ensure the 4π factor is explicitly included in the
inverse square law calculation (intensity = P / (4πr²)); (2) verify and ensure
the microphone surface area is set to a physically reasonable value (e.g., 1 cm²
or 0.0001 m²) rather than an unrealistically large value; (3) enforce a minimum
distance constraint of 0.2 meters between the microphone and any sound source as
a physical limit, meaning the arm's end effector position must be constrained
such that it cannot physically reach within 0.2 meters of any sound source
position, both during initialization and throughout the simulation; (4) implement
this physical constraint by modifying the arm's reachable workspace or by
constraining the forward kinematics to respect this minimum distance; and (5)
implement or enhance the test in test_energy_plausibility.py to verify that
cumulative sound energy remains physically plausible (i.e., total energy cannot
exceed source_power × simulation_time, and should be significantly less due to
distance attenuation, the 4π factor, and orientation effects).

~
Refactor and extend the agent system to support multiple tracking strategies:
(1) Refactor @HeuristicAgent to extract shared components (PD controller, target angle
computation, inverse kinematics) into a base class or shared utility functions that
can be reused across multiple agent implementations.
(2) Create three distinct agent classes with different tracking behaviors:
    - PointingAgent: Only orients the microphone toward the sound source without
      attempting to minimize distance (maintains current distance while pointing).
    - ApproachingAgent: Only tries to minimize distance to the sound source
      without explicit orientation control (the current HeuristicAgent behavior).
    - TrackingAgent: Both points toward and minimizes distance to the sound source,
      combining orientation and distance objectives.
(3) Update @demo.py to use the TrackingAgent instead of the current HeuristicAgent.
(4) Create @demo_all.py that runs all three agents simultaneously in separate
simulation instances, each with its own logging and animation visualization, allowing
side-by-side comparison of the different tracking strategies.
~
Update the logging of energy, reward and total reward to 4 positions after the decimal
point.
~
Update logging so that the specific simulation instance (e.g. those launched in
@demo_all) is identified according to the type of agent.

~
Normalize the reward signal to improve numerical stability and make it more suitable
for reinforcement learning algorithms. Currently, the reward is the sound energy delta,
which has very small numerical values (on the order of 1e-6 Joules per step).
Implement reward normalization as follows:
(1) Add a `reward_normalization_factor` parameter to @SoundFlowerConfig with a default
value of None (auto-computed). In the `__post_init__` method, if this factor is None,
automatically compute it as the theoretical maximum possible energy delta per step:
max_intensity = sound_source_strength / (4π × min_distance_to_source²)
max_energy_per_step = max_intensity × microphone_area × dt × 1.0
reward_normalization_factor = max_energy_per_step
(2) In @Environment.get_state(), normalize the reward by dividing sound_energy_delta
by the reward_normalization_factor before returning it in the State. This will scale
the reward to approximately [-1, 1] range (though it may exceed 1.0 if multiple
sources align perfectly or if the agent moves from zero to maximum energy in a single
step). Ensure the normalization factor is greater than zero before dividing to avoid
division by zero errors.

~
Add a plotter component for real-time visualization of simulation metrics, following
the design pattern of @logger and @animator. The plotter should:
(1) Use matplotlib for plotting and follow the same interface pattern as logger/animator
    (with __init__, start(), step(), and finish() methods) and integrate similarly
    into the simulation loop in @soundflower.py.
(2) Be designed as a shared instance that can track multiple simulation instances
    simultaneously (e.g., in @demo_all.py where multiple agents run in parallel).
    The plotter should accept an agent_name parameter to identify which simulation
    instance each data point belongs to.
(3) Create four separate plots that update in real-time as simulations progress:
    - Per-step reward (normalized)
    - Per-step sound energy (raw, in Joules)
    - Cumulative reward (sum of normalized rewards)
    - Cumulative sound energy (sum of raw energy, in Joules)
(4) For each plot, display multiple time series corresponding to different agents
    (e.g., PointingAgent, ApproachingAgent, TrackingAgent). Use distinct line colors
    and line styles (solid, dashed, dotted) to differentiate between agent types.
    Maintain a consistent color/style mapping across all four plots for the same agent.
(5) Handle normalization appropriately: use normalized reward values (already computed
    in the environment) for reward plots, and raw energy values for energy plots.
(6) Update plots incrementally as new data arrives (not batch updates at the end).
    Use matplotlib's interactive mode (plt.ion()) and refresh the plots at a
    configurable frequency (similar to animation_frequency in config). Ensure plots
    remain responsive and visible throughout the simulation run.
~
Pleaes also update @demo to use @plotter.

