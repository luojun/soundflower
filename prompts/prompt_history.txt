Please generate project files according to @prompt.txt .

~~
Please create a git ignore file appropriate for this project.

~~
Please add visualization. Please consider keeping the visualization code
in a separate package.

~~
Please actually do animation -- animated rendering of the environment
dynamics, including the robotic arm's motion and the sound source motion.
Maybe consider using Pygame?

~~
Currently the @agents/heuristic_agent.py implementation has the HeristicAgent
holding a reference to the environment object, which is less than ideal. It
needs that for the "run_episode" method. However, the run_episode method
really belongs elsewhere. Suggestion: please refactor the code such that we
 have an "experiments" package under which methods such as run_episode could
 live. Similarly, the repetitive code in @demo.py @demo_pygame.py and
 @demo_visualization.py could be refactored into an Experiment class under
 the experiments package and thus greatly simplfied.
~
Note that in @demo_pygame.py @demo.py and @demo_visualization.py ,
create_default_config was repeated three times. This method really belongs
to @experiments/experiment.py . Furthermore, please reconsider how experiment
level config should be used by the SoundFlowerEnvironment class and the
HeuristicAgent class. For now, it's probably OK to expose the config to them
directly. Please refactor further.

~~
We need to rectify how physics simulation is done, how it is coupled with
visualization, and how it is coupled with agent observation and action.
At a high level, the physics simulation cycle, the visualization cycle,
and the agent cycle (observation to decision) should all be asynchronoous.
These are then flexibly coupled under adjustable freqencies. The physics
simulation is to be governed by a changeable time step size, which is how
it discretize in time the underlying continuous physics. Then the simulation
frequency depends on how fast the simulation computation could run. This
frequency may be left unknown to be determined empirically on actual
computers. The visualization frequency needs to be suitable for human
observers, which means it is to be flexible in an acceptable range, such as
between 10fps and 100fps, the specific value could be picked according to
how fast the rendering engine (e.g. Pygame here) could run with respect to
the physics engine. I.e. there could be a somewhat flexible mapping between
fps and time step size of physical simulation. The agent interaction
frequency needs to be suitable for how fast the agent decision could run
(the mapping from observation to action). We can assume it's somewhere
between 10hz to 100hz. We can also call this the "control frequency".
Please refactor the code to (1) introduce a proper (2D) physics engine to
run the physical simulation with changeable time step size, (2) use
flexible control frequency to integrate agent's interaction with the 
physical environment, (3) allow a headless mode with integrated physics
simulation and agent control to run much faster than real time, and (4)
allow a flexible visualization frequency to watch the simulation at any
speed-up ratio that is reasonable.
~
Can you study this repo: https://github.com/luojun/OmegaZero/tree/master ?
To see whether there's anything to borrow from there? Maybe look into the
Runner, Renderer, and World (Environment) differentiation and interfadces? 

~~
Please (1) remove the version of visualization that uses matplotlib,
including demo_visualization.py and the underlying matplotlib implementation,
(2) remove demo_pygame.py along with the old pygame_visualizer.py,
(3) renaming the visulazation package to animation and change corresponding
namings for the implementation, and (4) rename demo_decoupled.py to
demo_animation.py.

~~
Why do you think we need both @demo.py and @demo_animation.py ? Can't we
use a single demo implementation? We already have a headless mode.

~~
Rename the @animation package to animator.

~~
Let's do a round of cleanup and refactoring: (1) Move create_default_config
in experiment.py into config.py in the soundflower package; (2) Remove 
experimemnt.py and simulation_experiment.py; (3) Rename the experiments
package into "experimenter"; (4) Move "config.py" and "runner.py" from 
the soundflower package into the renamed "experimenter" package; (5) Move 
the Observation dataclass from environment.py into world.py; (6) Remove
the old environment.py; (7) Create a new package called "environment";
(8) Move world.py, physics.py, and physics_engine.py into this new
environment package; (9) Move renderer.py into the animator package;
(10) Rename pygame_animator into pygame_framer, and adjust the references
to it as well as the classes it implements appriopriately.
~
Update README.md and ARCHITECTURE.md to reflect the current state of the code.
~
Remove soundflower/world.py and soundflower/config.py.
~
Remove simulation_loop.py, move world.py from the environment package
into the soundflower package, update all references in README.md if any.
Remove ARCHITECTURE.md, update all references to it if any.

#~~
#Did a bit of refactoring by hand to move Animator under Experimenter.
#~
#Refactoring by hand: renamed World into Environment and moved under environment.
#~
#Refactor by hand: simplify demo startup.
#~
#Refactor by hand: encapsulate animator initialization.
#~
#Refactor by hand: move experiment orchestration into runner.
#~
#Refacotr by hand: introduce logger and simplify integration by removing asyncio

~
Implement advanced simulation stepping features (through primarily changing @soundflower.py and @demo.py)
in the following way:
1. When the Return key is pressed, the simulation pauses and steps forward by
a single step, and logging and animation are all refreshed once by that one step.
After that, the simulation remains in a paused state.
2. When the '1' key is pressed, the behavior follows the same logic, except that the simulation
advances by 10 steps, and logging and animtation are all frefreshed once by that 10 steps.
3. When the '2' key is pressed, the same things, except that the simulation advances by 100 steps.
4. When the '3' key is pressed, the same thing by 1000 steps.
5. If the Space key is pressed while the simulation is paused, the whole simulation resumes normally
along with logging and animation, as how things are now.

~
Update the code to calculate and report performance better in the following way:
1. Replace wherever appropriate (e.g. in @physics.py) "sound_energy" by "sound_intensity";
2. Introduce specific converstion of sound_intensity to sound_energy by (2a) assuming
a unit area and (2b) multipilcation by time step size, i.e. dt.
3. Introduce orientation angle for the "microphone" (of the unit area) that receives
the sound such that the sound_intensity's conversion into sound_energy is a function of
the orientation angle, such as proportional to the cosine of difference between the
orientation angle and the direction of the sound source, or following whatever simple
but sensible physics. The orientation of the microphone should be in the direction of
the outmost link from the soundflower robot's base. Add up sound energy from multiple sources.
4. Introduce a new variable in @soundflower.py called "cumulative_sound_energy" and update it
according to the accumulation over time of sound_energy by the microphone.
5. Update logging for the final step to also report the cumulative_sound_energy.
6. Update logging per step to make the logged text easier to read with appropriate formatting
and include the simulation_time at that step as well as the step_count.
7. Remove simulation_time and step_count from @physics_engine.py and make sure that only
@soundflower.py tracks these.

~
Update the code to
1. Remove the Info part from get_state in @environment.py;
2. Make per step logging report reward rather than energy delta;
3. Use appropriate units for energy;
4. Make sure logging of float values use only two digits after the decimal point;
5. Remove _compute_distance_to_nearest_source and _compute_orientation_factors.

~
We need to fix the bug where the agent could accumulate more than 1 million Joules
of energy within 160 seconds even when the sound source is at most 2 Watts.
Concretely, (1) verify and ensure the 4π factor is explicitly included in the
inverse square law calculation (intensity = P / (4πr²)); (2) verify and ensure
the microphone surface area is set to a physically reasonable value (e.g., 1 cm²
or 0.0001 m²) rather than an unrealistically large value; (3) enforce a minimum
distance constraint of 0.2 meters between the microphone and any sound source as
a physical limit, meaning the arm's end effector position must be constrained
such that it cannot physically reach within 0.2 meters of any sound source
position, both during initialization and throughout the simulation; (4) implement
this physical constraint by modifying the arm's reachable workspace or by
constraining the forward kinematics to respect this minimum distance; and (5)
implement or enhance the test in test_energy_plausibility.py to verify that
cumulative sound energy remains physically plausible (i.e., total energy cannot
exceed source_power × simulation_time, and should be significantly less due to
distance attenuation, the 4π factor, and orientation effects).

~
Refactor and extend the agent system to support multiple tracking strategies:
(1) Refactor @HeuristicAgent to extract shared components (PD controller, target angle
computation, inverse kinematics) into a base class or shared utility functions that
can be reused across multiple agent implementations.
(2) Create three distinct agent classes with different tracking behaviors:
    - PointingAgent: Only orients the microphone toward the sound source without
      attempting to minimize distance (maintains current distance while pointing).
    - ApproachingAgent: Only tries to minimize distance to the sound source
      without explicit orientation control (the current HeuristicAgent behavior).
    - TrackingAgent: Both points toward and minimizes distance to the sound source,
      combining orientation and distance objectives.
(3) Update @demo.py to use the TrackingAgent instead of the current HeuristicAgent.
(4) Create @demo_all.py that runs all three agents simultaneously in separate
simulation instances, each with its own logging and animation visualization, allowing
side-by-side comparison of the different tracking strategies.
~
Update the logging of energy, reward and total reward to 4 positions after the decimal
point.
~
Update logging so that the specific simulation instance (e.g. those launched in
@demo_all) is identified according to the type of agent.

~
Normalize the reward signal to improve numerical stability and make it more suitable
for reinforcement learning algorithms. Currently, the reward is the sound energy delta,
which has very small numerical values (on the order of 1e-6 Joules per step).
Implement reward normalization as follows:
(1) Add a `reward_normalization_factor` parameter to @SoundFlowerConfig with a default
value of None (auto-computed). In the `__post_init__` method, if this factor is None,
automatically compute it as the theoretical maximum possible energy delta per step:
max_intensity = sound_source_strength / (4π × min_distance_to_source²)
max_energy_per_step = max_intensity × microphone_area × dt × 1.0
reward_normalization_factor = max_energy_per_step
(2) In @Environment.get_state(), normalize the reward by dividing sound_energy_delta
by the reward_normalization_factor before returning it in the State. This will scale
the reward to approximately [-1, 1] range (though it may exceed 1.0 if multiple
sources align perfectly or if the agent moves from zero to maximum energy in a single
step). Ensure the normalization factor is greater than zero before dividing to avoid
division by zero errors.

~
Add a plotter component for real-time visualization of simulation metrics, following
the design pattern of @logger and @animator. The plotter should:
(1) Use matplotlib for plotting and follow the same interface pattern as logger/animator
    (with __init__, start(), step(), and finish() methods) and integrate similarly
    into the simulation loop in @soundflower.py.
(2) Be designed as a shared instance that can track multiple simulation instances
    simultaneously (e.g., in @demo_all.py where multiple agents run in parallel).
    The plotter should accept an agent_name parameter to identify which simulation
    instance each data point belongs to.
(3) Create four separate plots that update in real-time as simulations progress:
    - Per-step reward (normalized)
    - Per-step sound energy (raw, in Joules)
    - Cumulative reward (sum of normalized rewards)
    - Cumulative sound energy (sum of raw energy, in Joules)
(4) For each plot, display multiple time series corresponding to different agents
    (e.g., PointingAgent, ApproachingAgent, TrackingAgent). Use distinct line colors
    and line styles (solid, dashed, dotted) to differentiate between agent types.
    Maintain a consistent color/style mapping across all four plots for the same agent.
(5) Handle normalization appropriately: use normalized reward values (already computed
    in the environment) for reward plots, and raw energy values for energy plots.
(6) Update plots incrementally as new data arrives (not batch updates at the end).
    Use matplotlib's interactive mode (plt.ion()) and refresh the plots at a
    configurable frequency (similar to animation_frequency in config). Ensure plots
    remain responsive and visible throughout the simulation run.
~
Pleaes also update @demo to use @plotter.

~
Refactor and improve the agent system to address critical issues in decision-making and
inverse kinematics. The current implementation has several problems that prevent agents
from effectively exploiting their degrees of freedom and differentiating their behaviors:

(1) Remove @HeuristicAgent as it is redundant. ApproachingAgent already implements the same
    behavior using the BaseAgent architecture. Update @test_energy_plausibility.py to use
    ApproachingAgent instead of HeuristicAgent, remove HeuristicAgent from @agents/__init__.py
    exports, and remove any references to HeuristicAgent in the plotter color/style mappings.

(2) Replace the simplistic inverse kinematics with proper position-based IK. The current
    `_compute_desired_joint_angles` method in @BaseAgent only distributes a target angle
    across joints with fixed ratios (0.7/0.3 for 2 links, 0.5/0.3/0.2 for 3 links), which
    doesn't actually solve for joint angles to reach a target position. This severely
    limits the agents' ability to exploit their degrees of freedom and causes all agents
    to behave similarly. Implement a proper Jacobian-based inverse kinematics solver in
    BaseAgent that:
    - Takes a target (x, y) position for the end effector
    - Uses numerical Jacobian computation and pseudo-inverse to solve for joint angles
    - Includes damping/regularization for stability
    - Works for both 2-link and 3-link arms
    - Has configurable convergence tolerance and maximum iterations

(3) Redesign agents to use position-based objectives instead of angle-based ones:
    - PointingAgent: Should optimize orientation toward the sound source while ignoring
      distance to target (distance is not subject to IK optimization pressure, though it
      may change naturally as a side effect). Compute target position by projecting the
      source direction onto a circle centered at base with radius equal to current
      end effector distance, then solve IK to reach that position.
    - ApproachingAgent: Should minimize distance to sound source while ignoring target
      orientation (orientation is not subject to IK optimization pressure, though it may
      change naturally as a side effect). Compute target position by moving toward the
      source with an adaptive step size (respecting minimum distance constraint), then
      solve IK to reach that position.
    - TrackingAgent: Should optimize both objectives simultaneously. Compute a weighted
      combination of the pointing target position and approaching target position, then
      solve IK to reach the combined target.

(4) Pass link_lengths to agents for IK computation. The IK solver needs link lengths to
    compute forward kinematics. Either add link_lengths as a parameter to BaseAgent.__init__
    (passed from config), or add a method to access them from the observation/environment.
    Ensure the IK solver uses the actual link lengths from the configuration.

(5) Consider additional improvements for better control:
    - Add adaptive PD gains that scale with distance to target or link lengths
    - For 3-link arms, consider null-space control to optimize secondary objectives
    - Add velocity feedforward terms for smoother motion
    - Consider joint limit constraints in the IK solver

These changes will enable agents to properly differentiate their behaviors, exploit the full
workspace, and scale effectively to 3-link arms while maintaining the shared code structure
in BaseAgent.

~
Fix a critical bug in the inverse kinematics solver where the first link (and potentially other
links) fails to move when its angle is between 180 and 360 degrees (i.e., pointing westward
through eastward). This bug affects all three agent types (PointingAgent, ApproachingAgent,
TrackingAgent) and prevents proper arm control in certain orientations. The root causes are:

(1) Angle clamping during IK iterations: In `_solve_inverse_kinematics` in @BaseAgent, angles
    are clamped to [-π, π] after each iteration (line 210: `angles = np.clip(angles, -np.pi, np.pi)`).
    This creates discontinuities when angles need to cross the ±π boundary. For example, if the
    current angle is 3.0 rad (172°) and needs to evolve to 3.5 rad (200°), the clamp wraps it
    to -2.78 rad, breaking the iterative convergence process.

(2) Missing angle wrapping in PD controller: In `_compute_pd_torques` in @BaseAgent, angle errors
    are computed without wrapping (line 229: `angle_errors = desired_angles - current_angles`).
    When angles wrap around the ±π boundary, this produces incorrect error values. For example,
    if desired_angle is -2.78 rad and current_angle is 3.0 rad, the error is -5.78 rad instead
    of the correct wrapped value of approximately 0.5 rad.

Implement the following fixes:

(1) Remove clamping during IK iterations: In `_solve_inverse_kinematics`, remove the `np.clip`
    call inside the iteration loop (line 210). Instead, normalize angles to [-π, π] only at the
    end of the function, after all iterations complete, using `angles = np.arctan2(np.sin(angles),
    np.cos(angles))`. This allows angles to evolve naturally during iterations without artificial
    discontinuities.

(2) Normalize input angles at IK start: At the beginning of `_solve_inverse_kinematics`, after
    copying `current_angles` to `angles`, normalize the initial angles to [-π, π] for consistency:
    `angles = np.arctan2(np.sin(angles), np.cos(angles))`.

(3) Add angle wrapping to PD controller: In `_compute_pd_torques`, after computing `angle_errors`,
    wrap the errors to [-π, π] using `angle_errors = np.arctan2(np.sin(angle_errors),
    np.cos(angle_errors))`. This ensures that angle errors are always computed as the shortest
    angular distance, correctly handling wrap-around cases.

These fixes will ensure that the IK solver can handle all angle ranges correctly and that the
PD controller computes appropriate torques regardless of the current joint angles' positions
relative to the ±π boundary.

~
Extend the inverse kinematics solver to explicitly support both position and orientation objectives,
and update all agents to use this unified interface. Energy maximization depends on both distance
(inverse square law) and orientation (cosine factor), so agents must optimize both independently or
in combination. Extend _solve_inverse_kinematics in @agents/base_agent.py to accept optional
target_orientation and weights (position_weight, orientation_weight), allowing position-only,
orientation-only, or combined control. Update PointingAgent to use orientation-only IK
(position_weight=0.0), ApproachingAgent to use position-only IK (orientation_weight=0.0),
and TrackingAgent to use both with appropriate weights. Rename variables in
@environment/environment.py for clarity where appropriate.
~
Update the argument order of _solve_inverse_kinematics and other related functions to a more
natural and appropriate order. Do review the variable names to see which ones should be updated
too. And no trailing whitespaces please.

~
Fix the issue where the keys '1', '2', '3', '4' no longer seem to have effect for @demo_all.
Note that these keys still works in @demo.

~
Fully implement a 3-link version of the environment, use it with demo.py, and update @demo_all
so that it runs 6 instances, namely the 3 agent types crossed with the 2-link and 3-link
versions of the environment.
~
Do not forget to update plotting if necessary to appropriately plot the performance curves
for all 6 instances simultaneously.

~
Optimize matplotlib plotting performance in @experimenter/plotter/plotter.py by replacing
full redraws with incremental updates. Current implementation clears and redraws all plots
every update (5 Hz default), causing 50-200ms delays.

Implement the following optimizations:
(1) Replace ax.clear() + plot() with line objects that use set_data() for incremental updates.
Store line objects in _line_objects dict keyed by agent_name and plot_type. Create lines on
first data point, then update with set_data() thereafter.
(2) Replace list-of-tuples data storage with numpy arrays (Tuple[np.ndarray, np.ndarray] for
x/y pairs) to avoid zip(*...) overhead. Limit data to MAX_DATA_POINTS=2000 per agent using
rolling window (keep last N points).
(3) Remove plt.tight_layout() from _update_plots() - call it only once in _initialize_plots().
(4) Replace canvas.draw() with canvas.draw_idle() for non-blocking updates, keeping
flush_events() only if needed.
(5) Update axes limits efficiently by computing min/max from current data arrays instead of
autoscaling.
(6) Only update legends when new agents are added (track _known_agents set,
set _legend_needs_update flag).
(7) Lower default plotting_frequency from 5.0 Hz to 0.5 Hz in @experimenter/config.py to
reduce update frequency.
(8) Increase legend font size from 7 to 10 and adjust ncol if needed for readability.

This should provide 20-100x speedup for plotting operations while maintaining visual quality.

~

Simplify agent implementations by removing redundant constraint handling and intermediate target computation.
The physics engine already enforces min_distance_to_source constraint in @environment/physics_engine.py
via _enforce_minimum_distance_constraint() called every step, so agents should not duplicate this logic.
Additionally, agents currently compute intermediate target_pos positions instead of optimizing directly
toward source_pos, causing workspace issues and unnecessary complexity. Refactor as follows: (1) Remove
min_distance_to_source parameter and all related logic from ApproachingAgent and TrackingAgent
constructors and select_action methods. The physics engine handles this constraint automatically.
(2) Remove intermediate target_pos computation - have ApproachingAgent optimize directly toward
source_pos using IK solver (target_pos=source_pos, position_weight=1.0, orientation_weight=0.0).
The IK solver naturally handles unreachable targets by converging to the closest reachable point.
(3) Simplify TrackingAgent to compute target_orientation from source_pos, then optimize both
position (target_pos=source_pos) and orientation simultaneously. Remove pointing_target and
approaching_target intermediate computations. (4) Update PointingAgent interface if needed to
maintain consistency, though its implementation may remain similar since it already uses
orientation-only optimization. (5) Remove min_distance_to_source from agent constructors in
@demo.py and @demo_all.py. This simplification fixes workspace bugs, removes redundant code, and
makes agents more direct and efficient.

~
Replace hard minimum distance constraint enforcement with soft spring-damper repulsion force and fix
torque clipping architecture. Current implementation causes violent arm oscillations because hard constraint
enforcement (adjusting arm position and resetting velocities) conflicts with agent control. Additionally,
torque clipping occurs at multiple levels including the PD controller, preventing agents from learning
about physical limits. Implement the following: (1) Replace _enforce_minimum_distance_constraint() hard
position adjustment with soft spring-damper repulsion force. When distance < min_distance_to_source,
compute repulsion force using spring-damper model: repulsion_force = k_repulsion * (min_distance - distance)
- c_damping * velocity_toward_source, where k_repulsion is spring coefficient (default 50.0) and
c_damping is damping coefficient (default 10.0). Compute velocity_toward_source by: (a) get end effector
velocity from joint velocities using position Jacobian: v_ee = J * q_dot, (b) project onto direction
toward source: velocity_toward_source = dot(v_ee, direction_to_source_normalized). Convert repulsion force
to joint torques using Jacobian transpose: repulsion_torques = J^T * repulsion_force_direction. Add
repulsion_torques to current_torques instead of adjusting arm position. Do not reset velocities. The
spring-damper model prevents crashes even with high velocities and is standard in robotics (virtual
fixtures). (2) Remove torque clipping from _compute_pd_torques() in @agents/base_agent.py - let PD
controller output freely. This is important for future RL learning where agents need to observe actual
control outputs. (3) Keep torque clipping only at physics boundary: @environment/environment.py
apply_action() and @environment/physics_engine.py set_torques(). Remove redundant clipping from
@environment/physics.py ArmPhysics.step() (line 100 hardcoded 10.0). (4) Replace "clamping" terminology
with "clipping" throughout codebase and comments (clipping is standard in control systems for limiting
values to a range). (5) Add repulsion_coefficient and repulsion_damping parameters to @experimenter/config.py
(defaults 50.0 and 10.0 respectively) to control soft constraint strength and damping. This approach
provides smooth repulsion with velocity-dependent damping, prevents crashes and violent oscillations,
enables RL learning, and maintains clean separation between control and physics.
~
Fix violent arm flapping caused by repulsion forces overwhelming agent control. The initial soft
spring-damper repulsion implementation caused excessive oscillations because repulsion forces were too
large relative to agent control torques, creating a feedback conflict. Implement the following fixes:
(1) Scale repulsion force relative to max_torque (30% limit) to prevent repulsion from dominating agent
control. Normalize violation by min_distance to get relative violation [0, 1] for consistent scaling.
(2) Clip repulsion torques to 30% of max_torque before adding to current_torques, ensuring repulsion
remains a soft constraint that supplements rather than overwhelms agent control. (3) Clip total torques
after adding repulsion torques to enforce physical limits. (4) Reduce default repulsion_coefficient from
50.0 to 10.0 and repulsion_damping from 10.0 to 5.0 in @experimenter/config.py for gentler, more stable
repulsion. These changes ensure repulsion acts as a soft constraint that guides the arm away from
violations while preserving goal-oriented agent behavior and preventing violent oscillations.

~
Increased mass and friction to smooth out the dynamics.

~
Implement multiprocessing for @demo_all.py to parallelize the 6 independent simulation instances. Each
SoundFlower instance should run in its own process, sending render data via multiprocessing.Queue to the
main process which handles Pygame rendering and matplotlib plotting. This provides true parallelism (bypasses
GIL) and scales with CPU cores. Keep @demo.py synchronous as threading provides minimal benefit for single
instances. Use process-safe data structures and ensure proper cleanup on exit.

~
Unify plotting interface across demos by creating an abstract Plotter base class with matplotlib and
TensorBoard implementations. Create @experimenter/plotter/plotter.py with abstract Plotter interface and
create_plotter() factory function. Move existing matplotlib plotter to @experimenter/plotter/matplotlib_plotter.py
and TensorBoard plotter to @experimenter/plotter/tensorboard_plotter.py, both inheriting from Plotter. Remove
"shared" argument from constructor. Update @demo.py to use create_plotter('matplotlib', ...) and @demo_all.py
to use create_plotter('tensorboard', ...). This provides unified interface while allowing different
implementations for different use cases (matplotlib for quick debugging, TensorBoard for scalable logging).

~
Implement unified PygameFramer that treats single-panel visualization as a 1×1 special case of multi-panel.
This eliminates code duplication in @demo_all.py by extending @experimenter/animator/pygame_framer.py to handle
multiple panels natively, while maintaining backward compatibility with single-panel usage in @demo.py.

~~
Ask
~
Now, we are about to embark on the journey of RL for this project. In preparation for that, we need to
rectify the agent interfaces. Specifically, (1) those aspects of what is assumed in the current
implementation as "robot body configurations", the robotic arm's link configurations, will need to be
turned into observations. This is so that bodily changes will become observable. Additionally, (2) the
"joint states", which the heuristic/control-theory-based agents currently use directly, should also
become observations. These should include the joint angle, joint angular velocity, and joint angular
acceleration. Thus, we have two groups of observations now. Could this be sensibly called
"geometricl observations" and "physical observations"? That is one of the things that need to be
reviewed and decided. The rationale of the distinction is that for an end-to-end RL agent, the
geometric observations which are currently used by the control-theoretic agents and depend on external
coordinate system should be banned and only physical observtions with local, i.e. egocentric-to-joint
observations should be allowed. Finally, (3) a third group of observations should also be available.
This group of "action observations" will be "efference copies" of the action commands, which
currently is joint torque. Please review this proposal vis-a-vis the code, especially regarding making
the current heuristic/control-theoretic agents use the new interfaces, as a test of the design and
integration. Please assess the reasonableness and potential pitfalls. And please recommend designs
and implementation choices for consideration.
~
Good considerations from a diversity of valid perspectives. Since the research is about scalable RL,
rather than taking shortcuts to make a system "perform", allowing heurstic agents to break down due to
partial observability and morphology change etc. is part of the point. It could make sense to
differentiate the observation modes as "egocentric" and "full". And it is indeed important to also
include sound power observation as part of the egocentric group. On the other hand, link mass, should
not be part of the egocentric group, it seems. Issues with normalization will likely become important.
But that will be handled later and hopefully in a prncipled way. Please synthesize all these into a
concretely fleshed out proposal.
~
Sound source direction and distance in egocentric coordinates should not be included in the egocentric
mode. The reason is that it assumes global geometry, even if the reference frame could be egocentric.
Whether they should be included in the Full mode, is a separate discussion. But with sound source position,
maybe that's unnecessary? Maybe the "egocentriic mode" should be called "basic mode"? Any better name?
Morphology should be in full mode, because we will want to check how the contro-theoretic agents perform
vis-a-vis the end-to-end RL agents as morphology changes.
~~
Agent
~
Let's go with "Sensorimotor", because this is a project that investigates sensorimotor abstraction.
As to whether convience features are needed, that can be decided as the Full interface gets used.
Now, please write up a prompt about the proposal and necessary implementation intrusctions and append
the prompt to @prompts/prompt_history.txt .
~
Please implement a two-mode observation interface for RL: "Sensorimotor" and "Full".
Sensorimotor mode should include only joint-local proprioception (angles, angular velocities, angular
accelerations), action efference copy (last applied joint torques), and a scalar sound power/intensity
observation. It must not include sound source direction or distance, nor any world-frame positions or
morphology parameters. Full mode should include Sensorimotor plus morphology parameters (link lengths,
masses, joint limits, frictions) and global information needed by control-theoretic agents (e.g., sound
source positions, end-effector/link positions). Expose a config flag (e.g., observation_mode) to select
the mode, update Observation/state construction accordingly, and refactor heuristic/control-theoretic
agents to use the new Observation fields rather than accessing environment internals. Expect heuristic
agents to degrade under Sensorimotor mode; this is acceptable and a useful test. Keep naming consistent
and avoid adding convenience features until Full mode usage reveals a need.
~~

~~
Ask
~
Now please examine implementation of the heuristic agents, for violation of our principled interfaces.
For example, link_lengths should not be passed as a constructor argument, but ratehr observation values.
The case of kp, kd, and max_torque is subtler. What are the choices here? max_torque certainly seems to
fit for an observation element. Should kp and kd be made observable? Or should they be part of the action
space? Or rather action space of some future meta-level controller that tunes kp and kd and thus here
only in observations? Or we leave kp and kd as agent parameters?
~~
Agent
~
Please go ahead propose the exact refactor. But please be thorough in reviewing the current state of
the code, because the four items discussed here may not be the only places where change is needed.
~
Remove max_torque from the agents.
~~

~~
Ask
~
Time for some RL fun. The next big step is the item on line 21 of @TODO.md .  We are to create an RL
agent that learns and works in the sensorimotor mode. There are many considerations. The first is
actually creating a heuristic agent that actually works at least to some extent in sensorimotor
mode, unlike the current control-theoretic agents, which simply fail. Here we may draw inspiration
from Braitenberg vehicles. Please design a Braitenberg-vehicle style heuristic agent that works in
the sensorimotor mode.
~
The cross-coupled mapping (Braitenberg "love" vehicle) and direct mapping options are good
ingredients. Could there be a design that is compatible with flexible number of joints? A single
design that works for 1 or 2 or 3 joints? Have people been inspired to do multi-DOF Braitenberg
robots?
~
Are phase-shifted oscillators a case of how central pattern generator is though to work?
~
Are the three options mentioned earlier, i.e. alternating sign, tapered pattern, and phase-shifted
oscillators, really variation of the same scheme? They are just transition matrices or rather a
"projecting vector" since the drive signal as sound intensity is a scalar. If we have a microphone
array here, we will have a transition matrix, right? Then the matrix will be how to control the
orientation and position of the microphone array. And if we are doing phase-shifteed 
for motor control, we will be doing that for a phase-array microphone setup. Is this understanding
correct?
~
What I was trying to say and said very cryptically is that the control of a microhone-phase-array
through modulating individual microphone's gain and the control of the multi-joint robotic arm on
which this microphone-array is mountd could be viewed as different aspects of the same control
problem. Thus, in a multi-DOF, multi-mic, and multi-source setup, we could do a joint control
scheme where the sum of individual mic's intensity delta is maximized.
~
Have people done something like this in practice or in research?
~~
Agent
~
Please do a bit of quick research on the single unified control formulation that explicitly 
(I almost want to say implicitly because the control channels do not need to be marked!) treats
joint torques and mic gains/phases as one action vector.
~
For now,  please summarize this bit as a topic in the form of a single to-do item under
the "Research Topic" section of @TODO.md . 
~~
Ask
~
Back to the robotic arm control  part, how is a joint treated from the oscillator phase perspective?
~
Thus, phase here is understood in terms of difference between desired amplitude in time and current
amplitude in time?
~
OK. Then, from the cotrol viewpoint, phase is the control knob.
~
Good. What is the movement cycle for a joint?
~
Can we treat linear actuators from this perspective too?
~
I see, like the piston-crankshaft stuff.
~
And we an linearize the sin function, then it's just simple dot product at work?
~
Fair enough. I'm concerned about minimal implementation, in which we do not necessarily want to
even have sin functions. I'm also thinking about learning. Possibly continual RL with linear maps.
The issue with basis features is certainly there.
~
Time to take stock. Please keep the above discussion around "Braitenberg-vehicle" style system in
mind, recommend specific options of a principled and simple archicture friendly to learning that
does reactive control under the sensorimotor mode without excessive control-theoretic elements.
~
Option 2 uses "eligibility trace"
~
I see. The memory elements here interact with A being non-diagonal and possibly non-symmetric.
~~
Agent
~
Cool. Please summarize Option 2 into a prompt and append to @prompts/prompt_history.txt .
It should summarize the rationale of Option 2. In the code that's to be generated, the code
naming and flow, as well as warranted comments, should make the rationale clear.
~
Please implement a minimal linear reactive controller as a baseline agent in the sensorimotor mode
with a small leaky memory state `z` (linear dynamics `z_{t+1}=A z_t + B x_t`, action `τ=W z_t`).
Rationale: stays sensorimotor-only, learning-friendly, and avoids explicit sinusoids; short-term
memory enables oscillation/coordination via non-diagonal `A`, while `A=λI` reduces to an
eligibility-trace-like decay. Ensure code naming, flow, and brief comments communicate this rationale.
Adjust the organization of the @agents package according to need. Add a new demo according to need.
~~
Ask
~
Tried the code with both 3 links and 2 links. The agent runs well in the beginning.
It was able to "capture" the sound source in the beginning. But once it loses the
orbiting sound source, it started flapping the robotic arm and is unable to recapture
it. For the 3-link case, this happens at around 45 seconds. For the 2-link case, it
happens at around 110 seconds. What could be going on?
~~
Agent
~
Indeed, please produce a minimal tweak and preserve the spirit. Generate a prompt and
append to @prompts/prompt_history.txt .
~
Please add a minimal tweak to the linear reactive baseline to prevent runaway flapping after
loss of the source while preserving the simple linear spirit. Keep it sensorimotor-only and
learning-friendly. Recommended minimal change: make A purely leaky (no rotational coupling)
and add a light damping term proportional to joint angular velocities directly in the policy
output, or decay/reset the memory state z when sound_intensity drops below a small threshold.
Avoid adding complex control logic, and keep code changes small and well-commented. Document
the choices with appropriate comments.
~~
Ask
~
It seems that the second choice is more RL friendly, because given that postive
sound_intensity delta is our choice of reward, it is only natural to let memory decay
when there is no adequate reward feedback. Keeping A as expresive a possible could be
more useful for linear RL later. Is this correct?
~~
Agent
~
Go ahead implement the decay option.
~
Ask
~
Somewhat better in that it seems to keep the source longer. And after losing the source,
it could still acquire some sound energy. But once losing the source, it could no longer
require it. This could be related to the fact that in the beginning the arm's momentum
was low. But once it starts moving, its momentum is high. It's almost as if when the
target is lost, the robot's mechanical memory should also decay.
~
Is it reasonable to have the reward_threshold being zero? Does it make sense for the
low_reward_decay and reward_threshold to be replace with a single, simple function
of reward? Such as the time derivative of normalized reward? Or the normalized
time derivative of the reward? Or something like that?
~
I'm thinking that maybe we should not attempt to introduce ad hoc complexity to make
the reactive agent behave better. What matters is that there is a reactive pathway,
that there is basic policy structure. The more principle approach is to use learning
and continually real time adaptation to keep the system stable and steady etc. And
eventually build out good spatial capability as sensorimotor abstraction.
~~

~~
Agent
~
Please do a round of deep research into linear continual RL. Swift SARSA is one possible
reference. Stream-x is another. But these all have nonlinear features, which we do not
even have now, but which we very likely will need later. We could learn from their
methods and architectural choices. We should also
be mindful of earlier linear RL literatures that may be relevant. What we want is an
architecture that could make our agent work well in adapting to environment variability
even with a simple linear core. Please go ahead with the deep research and come back
with a reviews and summaries of the more relevant options and possibly new options not
in the literature but could work in our context. Please describe each of these options
with their pros and cons. And make your recommendation as to which one or ones should
be taken seriously.
~
Please implement a linear continual RL baseline using linear TD(λ) with adaptive step sizes
(e.g., IDBD/AutoStep-style per-feature rates) plus a simple forgetting/decay mechanism to
handle non-stationarity. Keep the policy linear with a fixed A (linear memory dynamics)
and learn W online. Preserve sensorimotor-only observations. Provide a short note explaining
why this is the minimal, principled baseline for continual adaptation. If possible, make
this implementation extend the current or modified LinearReactiveAgent, so as to show
cleary how the old reactive core is in the new continual RL agent.

~~
Ask
~
We want to prepare for a bit of refactoring of the @agents package and the demos. This
is partly in preparation for the next phase of the project, which is to introduce
variability into the environment and the body.

Things to consider: (1) currently @BaseAgent assumes PD control, which should be updated to
be more compatible with all sorts of agent types; (2) the namings of specific agent
types follow two patterns, either behavior (pointing, tracking, and approaching) or
method (linear_reactive and continual_linear_rl) -- we need to clarify and unify; (3) the
@demo_all naming is no longer appropriate; (4) if we have a demo_all, which agents should
we include? all of them? (5) should we consider a demos package and leave only a single
primary demo that is very inviting and reflects the spirit of this project at the topic
level?

Please analyze the situation and propose options.
~
Use the currently best sensorimotor learner as the primary demo.py is a good idea.
The demo suite containing one controller agent (maybe the tracking one instead of the
pointing one? What are the pros and cons here?), one reactive agent,
and one learning agent is also a good idea. Thus, Option B seems to be largely good.
But not sure what "Exploration" means in "ExplorationLinearReactive" and
"ExplorationContinualRL". One consideration is to simply call these two "LinearReactor"
and "ExperentialLearner"/"LinearLearner". But please think and critique these choices
from multiple perspectives. One thing to note is that the control-theoretic agents
have a predefined "behavior" (Pointing, Tracking, or Approaching) but for the "Reactor"
and "Learner" that's not predefined.
~~
Agent
~
Let's go with LinearReactiveAgent and ContinualLinearRLAgent. Please write a prompt
for this refactoring that also includes what was decided above in the discussion
and append it below.
~
Please refactor @agents and demos with the following decisions:
1) Keep the method-based names for the non-behavior agents: `LinearReactiveAgent` and
   `ContinualLinearRLAgent`. Do not introduce "Exploration" naming.
2) Keep behavior-based names for PD/IK agents (Pointing/Tracking/Approaching), but
   remove PD assumptions from @BaseAgent by moving PD/IK helpers into a mixin or helper
   utility so that BaseAgent is a minimal interface for diverse agent types.
3) Rename @demo_all to a demo suite (e.g., `demo_suite.py`) and make a single inviting
   `demo.py` the primary entry point using the currently best sensorimotor learner.
4) The demo suite should include: one controller agent (consider Tracking vs Pointing,
   note pros/cons), one reactive agent, and one learning agent.
5) Keep the refactor minimal and coherent; update imports/exports and any docs to match.
~
Please create a `tests` package following Python testing conventions. Add tests for
each agent type, and move `test_energy_plausibility.py` into that package. Run the full
test suite to validate the refactor. Also run the demos in headless mode and verify
they execute without errors.
~~

~~
Ask
~
Let's start thinking about variability. The section titled "Body+Environment Variability"
starting on line 27 of @TODO.md has an initial list of things to consider. What we should
aim at is meaningful, realistic, and simple-to-implement variabilities that pose significant
challenges to an agent such that continual learning becomes essential. Please take to
heart the project's overall vision and trajectory: sensorimotor abstraction
supported by continual RL over sensorimotor contingency. Please also be very clear-headed
and open-minded in questioning and critiqueing any choices or options so that they are
principled. What are some variabilities that should be considered first? Grounded and
principled analaysis please.
~
Four more principles: (1) all changes or variabilities should be those that could happen
in the real-world in a what that is continuous in time without needing to reset or
restart the environment or the body; thus for example, the reduction of the number of
links in the arm better be treated as one joint becomes frozen; and in general, directional
drifting rather than random walk might be preferred, as well as something that is recognizably
"wear and tear"; (2) a heustic principle here is that the soundflower environment is a
cocktail party, with people (sound sources) coming and going, circling each other for a
while and then gone, or dashing around to say hello; (3) breaking control-theoretic
agents or any agent at all is precisely not a concern, if the variability is reasonable,
realisitc and meaingful -- that it may be challenging for particular approach just means
that approach is limited; (4) there is also a bit of UI consideration here in that
visualization should be natural enough and a human user should be able to tweak such
variability easily through GUI or keyboard.

With these and keeping the overall picture in mind, I would think that changing microphoe
gain is probably not warranted early on. Adding mild noise to any dynamics may be something
to consider, but it does not seem urgent. Time step and control frequency are important,
but they are less of a concern for now. The hope is that a continually learning agent
*should* be easier to tune or may not need tuning at all because it should not be overly
sensitive to the simulation parameters so long as they are generally reasonable.

What are the real first-priority variabilities then? And which other ones are worth putting
into the TODO list for later?
~
All very reasonable. Please turn this into a TODO list for @TODO.md
so that these variabilities and their UI choices are all listed.

~~
Agent
~
We are not ready to introduce some variabilities into the environment. Let's focus
on the **First** batch listed in @TODO.md. When we implement this first batch of
variabilities, we should also introduce the corresponding UI, for interactively
(1) changing the number of sources (1, 2, or 3), (2) changing the *range* of the
variability of the orbit radius, and (3) changing the *range* of the variability
of the orbiting speed, where we could use negative number to indicate
counterclockwise orbiting. Generate a succinct but clear prompt and append it to
the prompt_history file.
~
Implement the first batch of environment variabilities from TODO.md:
(1) variation in orbital velocity of sound sources (with slow drift over time),
(2) variation in orbit radius (slow drift), and (3) variation in number of active
sources (1 to 3). Add interactive UI controls to adjust in real-time:
(a) number of active sources (1, 2, or 3), (b) range of orbit radius variability
(min/max bounds), and (c) range of orbital speed variability (min/max bounds,
where negative values indicate counterclockwise rotation). The variabilities
should apply continuously during simulation without requiring resets, and the
UI should allow live adjustment of these parameters.
~~

~~
Agent
~
Generate a prompt and append it to prompt_history for the following:
(1) For the Pygame GUI part, please remove the current displayed text
for position, distance, and intensity. (2) Please increase the font
size for the remaining information text by 50%. (3) Please dim a
sound source and/or remove the pulsating animation when a source
is inactive.
~
Pygame GUI updates: (1) Remove the displayed text for position (Pos),
distance (Dist), and intensity (Intensity) from the info panel.
(2) Increase the font size of the remaining information text (agent
name, variability info) by 50%. (3) For inactive sound sources
(sources beyond the current number of active sources), dim their
appearance and/or remove the pulsating wave animation so that only
active sources are highlighted.
~
